# AI-Era Research Project Template

Project Context
âœ“ Era: NVIDIA "Stop Coding" - AI generates, humans orchestrate
âœ“ Project: [Project Name]
âœ“ Goal: [Leverage AI for breakthrough, maintain human oversight]
ğŸ‘¤ Team: Lead Researcher + AI Copilot Team
# Philosophy: Orchestrate AI, preserve judgment

---

Phase 1: AI-Assisted Problem Definition

Task 1.1: AI-Generate Research Questions
âœ“ Current: Broad domain interest
â–¶ Action: Use AI (GPT-4, Claude) to generate 20+ research questions
âš™ Implementation:
import openai

def generate_research_questions(domain, constraints):
    prompt = f"Generate novel AI research questions in {domain}"
    return openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
# Target: 20+ novel questions, 5+ with high feasibility
â˜‘ Validation: Human researcher selects top 3, AI ranks feasibility
ğŸ‘¤ Owner: Lead Researcher + AI Assistant
â±ï¸ 2 days
ğŸš© AI hallucination, derivative ideas
âŸ¿ â—‰ If 3+ promising questions â†’ Proceed
   â—‰ If none viable â†’ Human-led question formulation â†»

---

Task 1.2: AI-Augmented Literature Review
âœ“ Current: Manual review overwhelming
â–¶ Action: AI-assisted systematic review
âš™ Implementation:
# AI literature analyzer
def analyze_papers(papers):
    # Extract key insights, identify gaps
    # Cluster similar work
    # Generate citation graphs
    pass
# Target: 50+ papers reviewed, gaps mapped
â˜‘ Validation: Human spot-check 10% for accuracy
ğŸ‘¤ Owner: Researcher + AI Agent
â±ï¸ 1 week
ğŸš© AI misses seminal papers, bias in selection
âŸ¿ â—‰ If comprehensive â†’ Proceed
   â—‰ If gaps identified â†’ Expand search â†»

---

Phase 2: AI-Human Co-Development

Task 2.1: AI Prototype Generation
âœ“ Research question defined
â–¶ Action: Generate 3+ prototype implementations using AI
âš™ Implementation:
# Prompt to AI coding assistant:
"Implement a prototype for [research question] 
using [framework]. Include tests and evaluation."
# Target: 3 working prototypes, each <200 LOC
â˜‘ Validation: Human review for conceptual correctness
ğŸ‘¤ Owner: AI Agent (primary), Human (review)
â±ï¸ 3 days
ğŸš© Generated code buggy, conceptual errors
âŸ¿ â—‰ If 1+ viable prototype â†’ Human refinement
   â—‰ If all fail â†’ Adjust prompt/approach â†»

---

Task 2.2: Human-in-the-Loop Refinement
âœ“ AI-generated prototypes
â–¶ Action: Human refinement with AI assistance
âš™ Implementation:
# Human-AI pair programming session
# 1. Human identifies issues
# 2. AI suggests fixes
# 3. Human approves/rejects
# Target: Production-ready code, tests, documentation
â˜‘ Validation: Unit tests pass, performance benchmarks
ğŸ‘¤ Owner: Human (70%), AI (30%)
â±ï¸ 1 week
ğŸš© Over-reliance on AI, loss of understanding
âŸ¿ â—‰ If meets standards â†’ Evaluation phase
   â—‰ If quality issues â†’ More human intervention â†»

---

Phase 3: AI-Enhanced Evaluation

Task 3.1: Automated Experimentation
âœ“ Model implemented
â–¶ Action: AI-managed hyperparameter search and evaluation
âš™ Implementation:
# AutoML-style optimization
ai_experiment_runner = AILab(
    model=model,
    search_space=search_space,
    objective="accuracy",
    compute_budget="100 GPU-hours"
)
results = ai_experiment_runner.optimize()
# Target: Optimal configuration found, statistical significance
â˜‘ Validation: Human review of methodology, AI handles execution
ğŸ‘¤ Owner: AI Agent (execution), Human (design)
â±ï¸ 2 weeks (parallel compute)
ğŸš© Cost overruns, cherry-picked results
âŸ¿ â—‰ If statistically significant â†’ Writeup
   â—‰ If inconclusive â†’ Adjust experiment design â†»

---

Task 3.2: AI-Assisted Paper Writing
âœ“ Results validated
â–¶ Action: AI-assisted paper generation with human orchestration
âš™ Implementation:
1. AI drafts sections based on results
2. Human provides feedback, edits
3. AI revises, checks formatting
4. Human final approval
# Target: Conference-ready paper, reproducible results
â˜‘ Validation: AI plagiarism check, human peer review
ğŸ‘¤ Owner: Human (content), AI (editing/formatting)
â±ï¸ 2 weeks
ğŸš© AI-generated text lacks nuance, plagiarism
âŸ¿ â—‰ If accepted â†’ Conference + blog post
   â—‰ If rejected â†’ Human-led rewrite for next venue â†»

---

AI Era Research Principles
âœ“ Human defines direction, AI generates options
âœ“ AI executes at scale, human validates quality
âœ“ Symbolic workflows ensure auditability
âœ“ Risk management: AI errors caught by human gates
âœ“ Efficiency: 3x faster than traditional research
âœ“ Quality: Human judgment at critical points

Template Metadata
âœ“ Template: AI-Era Research Project
âœ“ Version: 3.0
âœ“ Philosophy: "Orchestrate AI, Preserve Judgment"
âœ“ License: MIT
